{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rename and copy folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the folder mappings\n",
    "folder_mappings = {\n",
    "    'processed_by_depgraph_withoutline': 'depgraph',\n",
    "    # 'processed_by_sbfl_withoutline': 'ochiai',\n",
    "    # 'processed_by_random_withoutline': 'execution'\n",
    "}\n",
    "\n",
    "# Define the base directory where 'RankedData' is located\n",
    "base_directory = '../data/RankedData'\n",
    "\n",
    "# Function to rename folders\n",
    "def rename_folders_in_directory(base_directory, folder_mappings):\n",
    "    # Iterate over all subdirectories in the base directory\n",
    "    for root, dirs, files in os.walk(base_directory):\n",
    "        for dir_name in dirs:\n",
    "            # Check if the current folder needs to be renamed\n",
    "            if dir_name in folder_mappings:\n",
    "                old_path = os.path.join(root, dir_name)\n",
    "                new_name = folder_mappings[dir_name]\n",
    "                new_path = os.path.join(root, new_name)\n",
    "                \n",
    "                # Rename the folder\n",
    "                print(f\"Renaming '{old_path}' to '{new_path}'\")\n",
    "                os.rename(old_path, new_path)\n",
    "\n",
    "# Run the renaming function\n",
    "rename_folders_in_directory(base_directory, folder_mappings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied and renamed /Users/user/Desktop/llmfl/llama-index-test/data/Time/processed_by_depgraph_withoutline to /Users/user/Desktop/llmfl/llm-order/data/RankedData/Time/depgraph\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# List of projects\n",
    "# projects = [\"Cli\", \"Math\", \"Csv\", \"Codec\", \"Gson\", \"JacksonCore\", \"JacksonXml\", \"Mockito\", \"Compress\", \"Jsoup\", \"Lang\"]\n",
    "projects = [\"Time\"]\n",
    "\n",
    "# Source and destination base paths\n",
    "source_base_path = \"/Users/user/Desktop/llmfl/llama-index-test/data\"\n",
    "destination_base_path = \"/Users/user/Desktop/llmfl/llm-order/data/RankedData\"\n",
    "\n",
    "# Loop through each project folder\n",
    "for project in projects:\n",
    "    source_folder = os.path.join(source_base_path, project, \"processed_by_depgraph_withoutline\")\n",
    "    destination_folder = os.path.join(destination_base_path, project, \"depgraph\")\n",
    "    \n",
    "    # Check if source folder exists\n",
    "    if os.path.exists(source_folder):\n",
    "        # Ensure the destination directory exists, if not, create it\n",
    "        os.makedirs(os.path.join(destination_base_path, project), exist_ok=True)\n",
    "        \n",
    "        # Copy the folder and rename it\n",
    "        try:\n",
    "            shutil.copytree(source_folder, destination_folder)\n",
    "            print(f\"Copied and renamed {source_folder} to {destination_folder}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error copying {source_folder} to {destination_folder}: {e}\")\n",
    "    else:\n",
    "        print(f\"Source folder {source_folder} does not exist.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put the Groundtruth in the First place of the ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def re_rank_methods(data):\n",
    "    for i, method in enumerate(data['covered_methods']):\n",
    "        method['method_id'] = i\n",
    "    return data\n",
    "\n",
    "\n",
    "# Function to map method IDs to their signatures from the JSON test files\n",
    "def map_method_ids_to_signatures(processed_dir, bug_id, test_id):\n",
    "    method_signatures_map = {}\n",
    "    test_file_path = os.path.join(processed_dir, f\"{bug_id}\", f\"test_{test_id}.json\")\n",
    "    if os.path.exists(test_file_path):\n",
    "        with open(test_file_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "        for method in data.get('covered_methods', []):\n",
    "            method_signatures_map[method['method_id']] = method['method_signature']\n",
    "    return method_signatures_map\n",
    "\n",
    "\n",
    "# Function to load the ground truth methods from the txt file for a specific bug\n",
    "def load_ground_truth_methods(ground_truth_dir, bug_id):\n",
    "    ground_truth_file = os.path.join(ground_truth_dir, f\"{bug_id}.txt\")\n",
    "    ground_truth_methods = set()  # Store ground truth methods in a set for quick lookup\n",
    "    if os.path.exists(ground_truth_file):\n",
    "        with open(ground_truth_file, 'r') as file:\n",
    "            ground_truth_methods = set(line.strip() for line in file.readlines())\n",
    "    return ground_truth_methods\n",
    "\n",
    "\n",
    "# Function to create a perfect ranking of covered methods, with ground truth methods at the top\n",
    "def generate_perfect_ranking(ranked_data_dir, ground_truth_dir, perfect_ranking_dir, project_name):\n",
    "    \n",
    "    if not os.path.exists(perfect_ranking_dir):\n",
    "        os.makedirs(perfect_ranking_dir)\n",
    "\n",
    "    for bug_id in os.listdir(ranked_data_dir):  # Loop over bugs in the project\n",
    "        bug_ranking_dir = os.path.join(perfect_ranking_dir, bug_id)\n",
    "        if not os.path.exists(bug_ranking_dir):\n",
    "            os.makedirs(bug_ranking_dir)\n",
    "\n",
    "        ground_truth_methods = load_ground_truth_methods(ground_truth_dir, bug_id)\n",
    "\n",
    "        for test_id in os.listdir(os.path.join(ranked_data_dir, bug_id)):\n",
    "            ranked_file_path = os.path.join(ranked_data_dir, bug_id, test_id)\n",
    "            perfect_ranking_file_path = os.path.join(bug_ranking_dir, test_id)\n",
    "            method_signatures_map = map_method_ids_to_signatures(ranked_data_dir, bug_id, test_id)\n",
    "\n",
    "            if os.path.exists(ranked_file_path):\n",
    "                with open(ranked_file_path, 'r') as json_file:\n",
    "                    data = json.load(json_file)\n",
    "                covered_methods = data.get('covered_methods', [])\n",
    "                \n",
    "                # Separate ground truth methods and non-ground truth methods\n",
    "                ground_truth_covered = []\n",
    "                non_ground_truth_covered = []\n",
    "\n",
    "                for method in covered_methods:\n",
    "                    if method['method_signature'] in ground_truth_methods:\n",
    "                        ground_truth_covered.append(method)\n",
    "                    else:\n",
    "                        non_ground_truth_covered.append(method)\n",
    "\n",
    "                # Update method ids: ground truth methods get highest ids\n",
    "                new_id = len(covered_methods)\n",
    "                for method in ground_truth_covered:\n",
    "                    method['method_id'] = new_id\n",
    "                    new_id -= 1\n",
    "\n",
    "                # Combine ground truth methods at the top, followed by other methods\n",
    "                perfect_covered_methods = ground_truth_covered + non_ground_truth_covered\n",
    "                data['covered_methods'] = perfect_covered_methods\n",
    "\n",
    "                data = re_rank_methods(data)\n",
    "\n",
    "                # Save the perfect ranking to the output file\n",
    "                with open(perfect_ranking_file_path, 'w') as json_file:\n",
    "                    json.dump(data, json_file, indent=4)\n",
    "\n",
    "\n",
    "# List of projects and techniques\n",
    "# projects = [\"Cli\", \"Math\", \"Csv\", \"Codec\", \"Gson\", \"JacksonCore\", \"JacksonXml\", \"Mockito\", \"Compress\", \"Jsoup\"]\n",
    "projects = [\"Time\"]\n",
    "techniques = [\"callgraph\"]\n",
    "\n",
    "for project_name in projects:\n",
    "    for technique in techniques:\n",
    "        ranked_data_dir = f'../data/RankedData/{project_name}/{technique}'\n",
    "        ground_truth_dir = f'../data/BuggyMethods/{project_name}'\n",
    "        perfect_ranking_dir = f'../data/RankedData/{project_name}/perfect_callgraph'\n",
    "        generate_perfect_ranking(ranked_data_dir, ground_truth_dir, perfect_ranking_dir, project_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomize the ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "def re_rank_methods(data):\n",
    "    \"\"\"Reassign method IDs in sequence after shuffling.\"\"\"\n",
    "    for i, method in enumerate(data['covered_methods']):\n",
    "        method['method_id'] = i\n",
    "    return data\n",
    "\n",
    "\n",
    "# Function to map method IDs to their signatures from the JSON test files\n",
    "def map_method_ids_to_signatures(processed_dir, bug_id, test_id):\n",
    "    method_signatures_map = {}\n",
    "    test_file_path = os.path.join(processed_dir, f\"{bug_id}\", f\"test_{test_id}.json\")\n",
    "    if os.path.exists(test_file_path):\n",
    "        with open(test_file_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "        for method in data.get('covered_methods', []):\n",
    "            method_signatures_map[method['method_id']] = method['method_signature']\n",
    "    return method_signatures_map\n",
    "\n",
    "\n",
    "# Function to create a random ranking of covered methods\n",
    "def generate_random_ranking(ranked_data_dir, random_ranking_dir, project_name):\n",
    "    \n",
    "    if not os.path.exists(random_ranking_dir):\n",
    "        os.makedirs(random_ranking_dir)\n",
    "\n",
    "    for bug_id in os.listdir(ranked_data_dir):  # Loop over bugs in the project\n",
    "        bug_ranking_dir = os.path.join(random_ranking_dir, bug_id)\n",
    "        if not os.path.exists(bug_ranking_dir):\n",
    "            os.makedirs(bug_ranking_dir)\n",
    "\n",
    "        for test_id in os.listdir(os.path.join(ranked_data_dir, bug_id)):\n",
    "            ranked_file_path = os.path.join(ranked_data_dir, bug_id, test_id)\n",
    "            random_ranking_file_path = os.path.join(bug_ranking_dir, test_id)\n",
    "\n",
    "            if os.path.exists(ranked_file_path):\n",
    "                with open(ranked_file_path, 'r') as json_file:\n",
    "                    data = json.load(json_file)\n",
    "                covered_methods = data.get('covered_methods', [])\n",
    "                \n",
    "                # Randomize the order of covered methods\n",
    "                random.shuffle(covered_methods)\n",
    "                data['covered_methods'] = covered_methods\n",
    "\n",
    "                # Reassign method IDs in sequence after randomizing\n",
    "                data = re_rank_methods(data)\n",
    "\n",
    "                # Save the random ranking to the output file\n",
    "                with open(random_ranking_file_path, 'w') as json_file:\n",
    "                    json.dump(data, json_file, indent=4)\n",
    "\n",
    "\n",
    "# List of projects and techniques\n",
    "# projects = [\"Cli\", \"Math\", \"Csv\", \"Codec\", \"Gson\", \"JacksonCore\", \"JacksonXml\", \"Mockito\", \"Compress\", \"Jsoup\", \"Lang\"]\n",
    "projects = [\"Time\"]\n",
    "techniques = [\"execution\"]\n",
    "\n",
    "for project_name in projects:\n",
    "    for technique in techniques:\n",
    "        ranked_data_dir = f'../data/RankedData/{project_name}/{technique}'\n",
    "        random_ranking_dir = f'../data/RankedData/{project_name}/random'  # Save random rankings here\n",
    "        generate_random_ranking(ranked_data_dir, random_ranking_dir, project_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the Kendall Tau Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kendall Tau Distance: -0.33333333333333337\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import kendalltau\n",
    "\n",
    "# JSON data\n",
    "json1 = {\n",
    "    \"covered_methods\": [\n",
    "        {\n",
    "            \"method_signature\": \"org.apache.commons.lang3.math.NumberUtils:createInteger(Ljava/lang/String;)Ljava/lang/Integer;\",\n",
    "            \"method_body\": \"public static Integer createInteger(final String str) {...}\",\n",
    "            \"method_id\": 0\n",
    "        },\n",
    "        {\n",
    "            \"method_signature\": \"org.apache.commons.lang3.math.NumberUtils:createNumber(Ljava/lang/String;)Ljava/lang/Number;\",\n",
    "            \"method_body\": \"public static Number createNumber(final String str) throws NumberFormatException {...}\",\n",
    "            \"method_id\": 1\n",
    "        },\n",
    "        {\n",
    "            \"method_signature\": \"org.apache.commons.lang3.StringUtils:isBlank(Ljava/lang/CharSequence;)Z\",\n",
    "            \"method_body\": \"public static boolean isBlank(final CharSequence cs) {...}\",\n",
    "            \"method_id\": 2\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "json2 = {\n",
    "    \"covered_methods\": [\n",
    "        {\n",
    "            \"method_signature\": \"org.apache.commons.lang3.math.NumberUtils:createNumber(Ljava/lang/String;)Ljava/lang/Number;\",\n",
    "            \"method_body\": \"public static Number createNumber(final String str) throws NumberFormatException {...}\",\n",
    "            \"method_id\": 0\n",
    "        },\n",
    "        {\n",
    "            \"method_signature\": \"org.apache.commons.lang3.StringUtils:isBlank(Ljava/lang/CharSequence;)Z\",\n",
    "            \"method_body\": \"public static boolean isBlank(final CharSequence cs) {...}\",\n",
    "            \"method_id\": 2\n",
    "        },\n",
    "        {\n",
    "            \"method_signature\": \"org.apache.commons.lang3.math.NumberUtils:createInteger(Ljava/lang/String;)Ljava/lang/Integer;\",\n",
    "            \"method_body\": \"public static Integer createInteger(final String str) {...}\",\n",
    "            \"method_id\": 1\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Extract the method signatures\n",
    "signatures1 = [method['method_signature'] for method in json1['covered_methods']]\n",
    "signatures2 = [method['method_signature'] for method in json2['covered_methods']]\n",
    "\n",
    "# Create rankings based on the method signatures\n",
    "# We map signatures1 to their ranking positions in signatures2\n",
    "ranking1 = [signatures1.index(sig) for sig in signatures1]\n",
    "ranking2 = [signatures2.index(sig) for sig in signatures1]  # Based on the order in signatures2\n",
    "\n",
    "# Calculate Kendall Tau Distance\n",
    "tau, p_value = kendalltau(ranking1, ranking2)\n",
    "\n",
    "print(f\"Kendall Tau Distance: {tau}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Kendal Tau Distance for all the ranked methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in JacksonXml, bug 5, file test_0.json: Size mismatch between rankings: 98 and 97\n",
      "Error in JacksonXml, bug 5, file test_0.json: Size mismatch between rankings: 98 and 97\n",
      "Error in JacksonXml, bug 5, file test_0.json: Size mismatch between rankings: 98 and 97\n",
      "Error in JacksonXml, bug 5, file test_0.json: Size mismatch between rankings: 98 and 97\n",
      "Results saved to ../data/KendallTau/combined_kendall_tau_results.json\n",
      "Error log saved to ../data/KendallTau/kendall_tau_errors.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "# Path to the Lang project containing bug versions and ranking data\n",
    "base_path = \"../data/RankedData\"\n",
    "\n",
    "projects = [\"Cli\", \"Math\", \"Csv\", \"Codec\", \"Gson\", \"JacksonCore\", \"JacksonXml\", \"Mockito\", \"Compress\", \"Jsoup\", \"Lang\"]\n",
    "\n",
    "# List of ranking techniques\n",
    "techniques = ['ochiai', 'depgraph', 'execution', 'perfect', 'random']\n",
    "\n",
    "# Dictionary to hold the results for all projects\n",
    "all_project_results = {}\n",
    "\n",
    "# Dictionary to keep track of errors (mismatched sizes)\n",
    "error_log = {}\n",
    "\n",
    "def load_ranking_data(project, version, technique, test_file):\n",
    "    \"\"\" Load the method_signature list from the JSON file for a given project, version, technique, and test file \"\"\"\n",
    "    file_path = os.path.join(base_path, project, technique, version, test_file)\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    # Extract the method_signature in the order they appear\n",
    "    return [method['method_signature'] for method in data['covered_methods']]\n",
    "\n",
    "def calculate_kendall_tau(project, technique1, technique2, version, test_file):\n",
    "    \"\"\" Calculate Kendall Tau Distance between two techniques for a given project, bug version, and test file \"\"\"\n",
    "    ranking1 = load_ranking_data(project, version, technique1, test_file)\n",
    "    ranking2 = load_ranking_data(project, version, technique2, test_file)\n",
    "    \n",
    "    # Check if there are fewer than two methods in either ranking\n",
    "    if len(ranking1) < 2 or len(ranking2) < 2:\n",
    "        return \"Insufficient data (less than 2 methods)\"\n",
    "    \n",
    "    # Create a list of method signatures and their respective rankings\n",
    "    rank_map1 = {signature: idx for idx, signature in enumerate(ranking1)}\n",
    "    rank_map2 = [rank_map1[signature] for signature in ranking2 if signature in rank_map1]\n",
    "\n",
    "    # Check if both ranking lists have the same size before calculation\n",
    "    if len(rank_map1) != len(rank_map2):\n",
    "        raise ValueError(f\"Size mismatch between rankings: {len(rank_map1)} and {len(rank_map2)}\")\n",
    "    \n",
    "    # Calculate Kendall Tau Distance if valid\n",
    "    if len(rank_map1) > 1 and len(rank_map2) > 1:\n",
    "        tau, p_value = kendalltau(list(rank_map1.values()), rank_map2)\n",
    "        return tau\n",
    "    else:\n",
    "        return \"Insufficient data (no valid pairs)\"\n",
    "\n",
    "\n",
    "# Loop through all projects\n",
    "for project in projects:\n",
    "    project_results = {}\n",
    "    \n",
    "    # List of bug versions by inspecting one technique (since the bug versions are common across all techniques)\n",
    "    bug_versions = os.listdir(f'{base_path}/{project}/{techniques[0]}')\n",
    "    \n",
    "    # Loop through all bug versions and calculate Kendall Tau Distance for all technique pairs\n",
    "    for version in bug_versions:\n",
    "        version_path = os.path.join(base_path, project, techniques[0], version)\n",
    "        \n",
    "        # Get all test files for this version (test_0.json, test_1.json, etc.)\n",
    "        test_files = [f for f in os.listdir(version_path) if f.endswith('.json')]\n",
    "        \n",
    "        version_key = f\"{version}\"\n",
    "        project_results[version_key] = {}  # Create an entry for this bug version\n",
    "        \n",
    "        # Loop through each test file (e.g., test_0.json, test_1.json)\n",
    "        for test_file in test_files:\n",
    "            test_key = f\"{test_file}\"\n",
    "            project_results[version_key][test_key] = {}  # Create an entry for this test file\n",
    "\n",
    "            # Compare all technique pairs for the current test file\n",
    "            for i in range(len(techniques)):\n",
    "                for j in range(i + 1, len(techniques)):\n",
    "                    technique1 = techniques[i]\n",
    "                    technique2 = techniques[j]\n",
    "                    \n",
    "                    # Calculate Kendall Tau Distance for the current test file\n",
    "                    try:\n",
    "                        tau = calculate_kendall_tau(project, technique1, technique2, version, test_file)\n",
    "                        comparison_key = f\"{technique1}_vs_{technique2}\"\n",
    "                        project_results[version_key][test_key][comparison_key] = tau\n",
    "                    except ValueError as e:\n",
    "                        print(f\"Error in {project}, bug {version}, file {test_file}: {e}\")\n",
    "                        \n",
    "                        # Log the error details in the error_log dictionary\n",
    "                        if project not in error_log:\n",
    "                            error_log[project] = {}\n",
    "                        if version not in error_log[project]:\n",
    "                            error_log[project][version] = {}\n",
    "                        if test_file not in error_log[project][version]:\n",
    "                            error_log[project][version][test_file] = []\n",
    "                        \n",
    "                        error_log[project][version][test_file].append(f\"{technique1}_vs_{technique2}: {str(e)}\")\n",
    "\n",
    "    # Store the project results in the main dictionary\n",
    "    all_project_results[project] = project_results\n",
    "\n",
    "# Save the combined results for all projects to a JSON file\n",
    "output_path = \"../data/KendallTau/combined_kendall_tau_results.json\"\n",
    "with open(output_path, 'w') as outfile:\n",
    "    json.dump(all_project_results, outfile, indent=4)\n",
    "\n",
    "# Save the error log to a separate JSON file\n",
    "error_log_path = \"../data/KendallTau/kendall_tau_errors.json\"\n",
    "with open(error_log_path, 'w') as errorfile:\n",
    "    json.dump(error_log, errorfile, indent=4)\n",
    "\n",
    "print(f\"Results saved to {output_path}\")\n",
    "print(f\"Error log saved to {error_log_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate More Metrics from the overall Kendall Tau distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary saved to ../data/KendallTau/kendall_tau_summary.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Load the combined results from the JSON file\n",
    "with open('../data/KendallTau/combined_kendall_tau_results.json', 'r') as f:\n",
    "    all_project_results = json.load(f)\n",
    "\n",
    "# Dictionary to hold the summary results\n",
    "project_summary = {}\n",
    "\n",
    "def calculate_project_summary(project, project_data):\n",
    "    \"\"\" Calculate the overall average and per-pair averages for a project \"\"\"\n",
    "    technique_pairs = set()\n",
    "    tau_values = {}\n",
    "    \n",
    "    # Initialize tau values for each technique pair\n",
    "    for version, version_data in project_data.items():\n",
    "        for test_file, test_data in version_data.items():\n",
    "            for pair, tau in test_data.items():\n",
    "                if isinstance(tau, (int, float)):  # Only consider valid numerical values\n",
    "                    if pair not in tau_values:\n",
    "                        tau_values[pair] = []\n",
    "                    tau_values[pair].append(tau)\n",
    "    \n",
    "    # Calculate the overall averages and technique-pair-specific averages\n",
    "    overall_tau_values = []\n",
    "    pair_averages = {}\n",
    "    \n",
    "    for pair, values in tau_values.items():\n",
    "        avg_tau = np.mean(values)\n",
    "        pair_averages[pair] = avg_tau\n",
    "        overall_tau_values.extend(values)\n",
    "    \n",
    "    overall_avg = np.mean(overall_tau_values)\n",
    "    overall_std = np.std(overall_tau_values)\n",
    "    \n",
    "    return {\n",
    "        \"overall_average_tau\": overall_avg,\n",
    "        \"overall_std_tau\": overall_std,\n",
    "        \"pair_averages\": pair_averages,\n",
    "        \"total_comparisons\": len(overall_tau_values)\n",
    "    }\n",
    "\n",
    "# Loop through each project in the results\n",
    "for project, project_data in all_project_results.items():\n",
    "    project_summary[project] = calculate_project_summary(project, project_data)\n",
    "\n",
    "# Save the summary to a JSON file\n",
    "summary_output_path = \"../data/KendallTau/kendall_tau_summary.json\"\n",
    "with open(summary_output_path, 'w') as outfile:\n",
    "    json.dump(project_summary, outfile, indent=4)\n",
    "\n",
    "print(f\"Summary saved to {summary_output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rename the folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed: ../data/RankedData/Compress/callgraph -> ../data/RankedData/Compress/callgraph_dfs\n",
      "Renamed: ../data/RankedData/JacksonCore/callgraph -> ../data/RankedData/JacksonCore/callgraph_dfs\n",
      "Renamed: ../data/RankedData/Gson/callgraph -> ../data/RankedData/Gson/callgraph_dfs\n",
      "Renamed: ../data/RankedData/JacksonXml/callgraph -> ../data/RankedData/JacksonXml/callgraph_dfs\n",
      "Renamed: ../data/RankedData/Codec/callgraph -> ../data/RankedData/Codec/callgraph_dfs\n",
      "Renamed: ../data/RankedData/Cli/callgraph -> ../data/RankedData/Cli/callgraph_dfs\n",
      "Renamed: ../data/RankedData/Math/callgraph -> ../data/RankedData/Math/callgraph_dfs\n",
      "Renamed: ../data/RankedData/Time/callgraph -> ../data/RankedData/Time/callgraph_dfs\n",
      "Renamed: ../data/RankedData/Lang/callgraph -> ../data/RankedData/Lang/callgraph_dfs\n",
      "Renamed: ../data/RankedData/Jsoup/callgraph -> ../data/RankedData/Jsoup/callgraph_dfs\n",
      "Renamed: ../data/RankedData/Csv/callgraph -> ../data/RankedData/Csv/callgraph_dfs\n",
      "Renamed: ../data/RankedData/Mockito/callgraph -> ../data/RankedData/Mockito/callgraph_dfs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def rename_callgraph_folders(base_path):\n",
    "    \"\"\"\n",
    "    Renames all 'callgraph' folders to 'callgraph_dfs' within project folders under the base path.\n",
    "\n",
    "    Args:\n",
    "        base_path (str): The path to the root folder containing project directories.\n",
    "    \"\"\"\n",
    "    # Iterate through all project folders in the base path\n",
    "    for project_name in os.listdir(base_path):\n",
    "        project_path = os.path.join(base_path, project_name)\n",
    "\n",
    "        # Ensure it's a directory\n",
    "        if os.path.isdir(project_path):\n",
    "            callgraph_path = os.path.join(project_path, \"callgraph\")\n",
    "            new_callgraph_path = os.path.join(project_path, \"callgraph_dfs\")\n",
    "\n",
    "            # Check if the 'callgraph' folder exists and rename it\n",
    "            if os.path.exists(callgraph_path):\n",
    "                os.rename(callgraph_path, new_callgraph_path)\n",
    "                print(f\"Renamed: {callgraph_path} -> {new_callgraph_path}\")\n",
    "            else:\n",
    "                print(f\"No 'callgraph' folder found in {project_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the base path\n",
    "    base_path = \"../data/RankedData\"\n",
    "\n",
    "    # Run the renaming script\n",
    "    rename_callgraph_folders(base_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Kendall Tau distance between two rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Kendall Tau distances by project:\n",
      "\n",
      "Project: Cli\n",
      "  Perfect vs callgraph_bfs: 0.6704479015828727\n",
      "  Perfect vs callgraph_dfs: 0.934192255470955\n",
      "\n",
      "Project: Math\n",
      "  Perfect vs callgraph_bfs: 0.8168682422221175\n",
      "  Perfect vs callgraph_dfs: 0.9619466402996407\n",
      "\n",
      "Project: Csv\n",
      "  Perfect vs callgraph_bfs: 0.8389668386472428\n",
      "  Perfect vs callgraph_dfs: 0.9825531198753418\n",
      "\n",
      "Project: Codec\n",
      "  Perfect vs callgraph_bfs: 0.7062768308489742\n",
      "  Perfect vs callgraph_dfs: 0.8461477696256832\n",
      "\n",
      "Project: Compress\n",
      "  Perfect vs callgraph_bfs: 0.8663448839935057\n",
      "  Perfect vs callgraph_dfs: 0.9836811651895877\n",
      "\n",
      "Project: Gson\n",
      "  Perfect vs callgraph_bfs: 0.8491498040527607\n",
      "  Perfect vs callgraph_dfs: 0.9469443643012886\n",
      "\n",
      "Project: JacksonCore\n",
      "  Perfect vs callgraph_bfs: 0.8467787286965645\n",
      "  Perfect vs callgraph_dfs: 0.9409041183606812\n",
      "\n",
      "Project: JacksonXml\n",
      "  Perfect vs callgraph_bfs: 0.9379582875532803\n",
      "  Perfect vs callgraph_dfs: 0.9961593457543384\n",
      "\n",
      "Project: Mockito\n",
      "  Perfect vs callgraph_bfs: 0.9839779198698787\n",
      "  Perfect vs callgraph_dfs: 0.9941003523419262\n",
      "\n",
      "Project: Jsoup\n",
      "  Perfect vs callgraph_bfs: 0.9235295224510538\n",
      "  Perfect vs callgraph_dfs: 0.9543336613831023\n",
      "\n",
      "Project: Lang\n",
      "  Perfect vs callgraph_bfs: 0.797558055274991\n",
      "  Perfect vs callgraph_dfs: 0.887460283179409\n",
      "\n",
      "Project: Time\n",
      "  Perfect vs callgraph_bfs: 0.9151914601287808\n",
      "  Perfect vs callgraph_dfs: 0.994408854123663\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from scipy.stats import kendalltau\n",
    "from glob import glob\n",
    "\n",
    "def load_signatures(file_path):\n",
    "    \"\"\"Load the method signatures in the order they appear in the file.\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return [method[\"method_signature\"] for method in data[\"covered_methods\"]]\n",
    "\n",
    "def calculate_kendall_tau(perfect_signatures, other_signatures):\n",
    "    \"\"\"Calculate Kendall Tau distance between two rankings based on method signatures.\"\"\"\n",
    "    # Map signatures to indices in the perfect ranking\n",
    "    perfect_order = [perfect_signatures.index(sig) for sig in perfect_signatures]\n",
    "    other_order = [perfect_signatures.index(sig) for sig in other_signatures if sig in perfect_signatures]\n",
    "\n",
    "    # Check if lengths match\n",
    "    if len(perfect_order) != len(other_order):\n",
    "        return float('nan')\n",
    "\n",
    "    # Handle the case of a single covered method\n",
    "    if len(perfect_order) == 1:\n",
    "        return 1.0  # Perfect agreement by default\n",
    "\n",
    "    # Calculate Kendall Tau\n",
    "    tau, _ = kendalltau(perfect_order, other_order)\n",
    "    return tau\n",
    "\n",
    "def compare_with_perfect(perfect_dir, comparison_dirs):\n",
    "    \"\"\"Compare rankings in other directories with the perfect ranking.\"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # Iterate over version folders in the perfect directory\n",
    "    version_dirs = glob(os.path.join(perfect_dir, \"*\"))  # Example: `perfect_callgraph/1`, `perfect_callgraph/2`\n",
    "    \n",
    "    for version_dir in version_dirs:\n",
    "        version = os.path.basename(version_dir)\n",
    "        perfect_files = glob(os.path.join(version_dir, \"test_*.json\"))\n",
    "        \n",
    "        for perfect_file in perfect_files:\n",
    "            perfect_filename = os.path.basename(perfect_file)\n",
    "            perfect_signatures = load_signatures(perfect_file)\n",
    "\n",
    "            for comparison_dir in comparison_dirs:\n",
    "                comparison_name = os.path.basename(comparison_dir)\n",
    "                if comparison_name not in results:\n",
    "                    results[comparison_name] = []\n",
    "\n",
    "                # Locate the corresponding version and file in the comparison directory\n",
    "                comparison_file = os.path.join(comparison_dir, version, perfect_filename)\n",
    "\n",
    "                if os.path.exists(comparison_file):\n",
    "                    other_signatures = load_signatures(comparison_file)\n",
    "                    tau = calculate_kendall_tau(perfect_signatures, other_signatures)\n",
    "                    results[comparison_name].append(tau)\n",
    "\n",
    "    return results\n",
    "\n",
    "def calculate_average_kendall_tau(results):\n",
    "    \"\"\"Calculate the average Kendall Tau distance for each comparison.\"\"\"\n",
    "    averages = {}\n",
    "    for technique, taus in results.items():\n",
    "        valid_taus = [tau for tau in taus if tau is not None]  # Filter out None values\n",
    "        if valid_taus:\n",
    "            averages[technique] = sum(valid_taus) / len(valid_taus)  # Compute average\n",
    "        else:\n",
    "            averages[technique] = float('nan')  # Handle case with no valid results\n",
    "    return averages\n",
    "\n",
    "# Define paths and projects\n",
    "projects = [\"Cli\", \"Math\", \"Csv\", \"Codec\", \"Compress\", \"Gson\", \"JacksonCore\", \"JacksonXml\", \"Mockito\", \"Jsoup\", \"Lang\", \"Time\"]\n",
    "base_path = \"../data/RankedData\"\n",
    "\n",
    "# Process each project and print results\n",
    "print(\"\\nOverall Kendall Tau distances by project:\")\n",
    "for project in projects:\n",
    "    perfect_dir = os.path.join(base_path, project, \"perfect_callgraph\")\n",
    "    comparison_dirs = [\n",
    "        os.path.join(base_path, project, \"callgraph_bfs\"),\n",
    "        os.path.join(base_path, project, \"callgraph_dfs\")\n",
    "    ]\n",
    "\n",
    "    if os.path.exists(perfect_dir):\n",
    "        distances = compare_with_perfect(perfect_dir, comparison_dirs)\n",
    "        averages = calculate_average_kendall_tau(distances)\n",
    "\n",
    "        # Print the results for the current project\n",
    "        print(f\"\\nProject: {project}\")\n",
    "        for technique, avg_tau in averages.items():\n",
    "            print(f\"  Perfect vs {technique}: {avg_tau}\")\n",
    "    else:\n",
    "        print(f\"\\nProject: {project} - Perfect directory not found.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-order-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
